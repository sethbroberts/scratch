{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial, reduce\n",
    "# from linear_algebra import dot, vector_add\n",
    "# from gradient_descent import maximize_stochastic, maximize_batch\n",
    "# from working_with_data import rescale\n",
    "# from machine_learning import train_test_split\n",
    "# from multiple_regression import estimate_beta, predict\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def safe(f):\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f\n",
    "\n",
    "def negate(f):\n",
    "    # negates a fnc that returns a value\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    # negates a fnc that returns a list\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn), negate_all(gradient_fn), theta_0, tolerance)\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "        # choose next theta by minimizing error\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        # stop if converging\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "            \n",
    "            \n",
    "def split(data, train_fraction):\n",
    "    results = [], [] # train, test\n",
    "    for row in data:\n",
    "        results[0 if random.random() < train_fraction else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def train_test_split(x, y, train_fraction=0.66):\n",
    "    data = zip(x,y)\n",
    "    train, test = split(data, train_fraction)\n",
    "    x_train, y_train = list(zip(*train))\n",
    "    x_test, y_test = list(zip(*test))\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_add(v, w):\n",
    "    \"\"\"adds two vectors componentwise\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"assumes x has at least two elements\"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "\n",
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "def get_row(A, i):\n",
    "    return A[i]\n",
    "\n",
    "def get_column(A, j):\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"returns a num_rows x num_cols matrix\n",
    "    whose (i,j)-th entry is entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i, j) for j in range(num_cols)]\n",
    "            for i in range(num_rows)]\n",
    "\n",
    "\n",
    "def scale(data_matrix):\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    means = [mean(get_column(data_matrix,j))\n",
    "             for j in range(num_cols)]\n",
    "    stdevs = [standard_deviation(get_column(data_matrix,j))\n",
    "              for j in range(num_cols)]\n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data_matrix):\n",
    "    \"\"\"rescales the input data so that each column\n",
    "    has mean 0 and standard deviation 1\n",
    "    ignores columns with no deviation\"\"\"\n",
    "    means, stdevs = scale(data_matrix)\n",
    "\n",
    "    def rescaled(i, j):\n",
    "        if stdevs[j] > 0:\n",
    "            return (data_matrix[i][j] - means[j]) / stdevs[j]\n",
    "        else:\n",
    "            return data_matrix[i][j]\n",
    "\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    return make_matrix(num_rows, num_cols, rescaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n",
    "\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))\n",
    "\n",
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))\n",
    "\n",
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"here i is the index of the data point,\n",
    "    j the index of the derivative\"\"\"\n",
    "\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"the gradient of the log likelihood\n",
    "    corresponding to the i-th data point\"\"\"\n",
    "\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "            for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "                  [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                   for x_i, y_i in zip(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logreg_inputs\n",
    "\n",
    "# each element is [1, experience, salary]\n",
    "data = logreg_inputs.DATA\n",
    "x = [[1] + row[:2] for row in data]\n",
    "y = [row[2] for row in data]\n",
    "rescaled_x = rescale(x)\n",
    "\n",
    "random.seed(0)\n",
    "x_train, y_train, x_test, y_test = train_test_split(rescaled_x, y, train_fraction=0.66)\n",
    "\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "grad_fn = partial(logistic_log_gradient, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 [0.44535218971882984, 0.25922692344722276, 0.15768657212231085]\n",
      "beta_batch [-1.8935557020921057, 4.0148875697806305, -3.8443576079115345]\n"
     ]
    }
   ],
   "source": [
    "# pick a random starting point\n",
    "beta_0 = [random.random() for _ in range(3)]\n",
    "\n",
    "# and maximize using gradient descent\n",
    "beta_hat = maximize_batch(fn, grad_fn, beta_0)\n",
    "\n",
    "print(\"beta_0\", beta_0)\n",
    "print(\"beta_batch\", beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzz(k):\n",
    "    return k * random.gauss(0,1)\n",
    "\n",
    "def dot(v1, v2):\n",
    "    return sum([(v1i*v2i) for v1i, v2i in zip(v1, v2)])\n",
    "\n",
    "def vector_add(v1, v2):\n",
    "    return [(v1_i + v2_i) for v1_i, v2_i in zip(v1, v2)]\n",
    "\n",
    "def logistic(a):\n",
    "    return 1 / (1 + math.exp(-a))\n",
    "\n",
    "def make_data(true_params, n=1000, k=0.7):\n",
    "    X = [[random.uniform(-3,3) for _ in range(len(true_params)-1)] + [1] for _ in range(n)]\n",
    "    N = [dot(x, true_params) for x in X]\n",
    "    S = [logistic(n) + fuzz(k) for n in N]\n",
    "    Y = [1 if random.random() <= s else 0 for s in S]\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(0)\n",
    "beta_true = [4, -3, -1]\n",
    "beta_0 = [random.random() for _ in range(len(beta_true))]\n",
    "\n",
    "x, y = make_data(beta_true)\n",
    "\n",
    "rescaled_x = rescale(x)\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(rescaled_x, y, train_fraction=0.66)\n",
    "\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "grad_fn = partial(logistic_log_gradient, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_batch [0.7057577613714192, -0.40935071705217896, -0.13842910852034962]\n"
     ]
    }
   ],
   "source": [
    "# and maximize using gradient descent\n",
    "beta_hat = maximize_batch(fn, grad_fn, beta_0)\n",
    "\n",
    "print(\"beta_batch\", beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0, -2.3200635654745563, -0.7845700952766342]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(beta_true[0] / beta_hat[0]) * bi for bi in beta_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
